{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from itertools import combinations\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "MAGIC_SQUARE = [2, 7, 6, \n",
    "                9, 5, 1, \n",
    "                4, 3, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['x', 'o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win(elements):\n",
    "    return any(sum(c) == 15 for c in combinations(elements, 3))\n",
    "\n",
    "\n",
    "def winning_state(pos: State):\n",
    "    if win(pos.x):\n",
    "        return 1\n",
    "    elif win(pos.o):\n",
    "        return -1\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "\n",
    "def available_moves(game_state: State):\n",
    "    return [i for i in range(1, 10) if i not in game_state.x and i not in game_state.o]\n",
    "\n",
    "\n",
    "def print_board(state: State):\n",
    "    for row in range(3):\n",
    "        for col in range(3):\n",
    "            index = row * 3 + col\n",
    "            if MAGIC_SQUARE[index] in state.x:\n",
    "                print('x ', end='')\n",
    "            elif MAGIC_SQUARE[index] in state.o:\n",
    "                print('o ', end='')\n",
    "            else:\n",
    "                print('- ', end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q_learning_player\n",
    "The Reinforced learning player is characterized by 3 factors that help it make choices and adapt\n",
    "- *learning rate* determines the impact of newly acquired informations compared to the old ones: at 0 the system doesn't learn anything, at 1 the system only considers the last acquired information\n",
    "- *discount factor* determines the impact of future rewards: at 0 the system only considers immediate reward (greedy), as the value increases the system considers more and more future rewards\n",
    "- *exploration_rate* determines the chance the system will explore the environment rather than exploiting it (acts as a percentage chance)\n",
    "\n",
    "The Q_learning_player decides every move wether to explore (make a random move) or exploit (make a move it knows the reward for). For exploiting it keeps a dictionary of all encountered game states, and to each state corresponds a list possible moves from that state and a list containing the associated rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "class random_player():\n",
    "    def make_move(self, game_state: State):\n",
    "        return random.choice(available_moves(game_state))\n",
    "    \n",
    "    def name(self):\n",
    "        return 'Random_player'\n",
    "\n",
    "\n",
    "class Q_learning_player():\n",
    "    def __init__(self, learning_rate, discount_factor, exploration_rate, role):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.learning_dictionary = defaultdict(int)\n",
    "        self.role = role\n",
    "        # because the rewards are set up to be positive if x wins and negative if o wins, \n",
    "        # the player must know if it's playing as x or o to know if it must maximize on minimize the rewards\n",
    "\n",
    "    def make_move(self, game_state: State):\n",
    "        hashable_state = (frozenset(game_state.x), frozenset(game_state.o))\n",
    "        if random.uniform(0, 1) > self.exploration_rate and self.learning_dictionary[hashable_state] != 0:\n",
    "            # exploit, aka choose a move based on what it has learned so far\n",
    "            moves, rewards = self.learning_dictionary[hashable_state] # get a list of possible moves from the current state\n",
    "            if self.role == 'x':\n",
    "                best_move_index = rewards.index(max(rewards))\n",
    "            else:\n",
    "                best_move_index = rewards.index(min(rewards))\n",
    "            return moves[best_move_index]\n",
    "        else:\n",
    "            # explore, aka make a new random move outside of what it has learned so far\n",
    "            return random.choice(available_moves(game_state))\n",
    "\n",
    "    def update(self, game_log, reward):\n",
    "        for game_state, move, next_state in game_log: # for each move it took during the game\n",
    "\n",
    "            hashable_state = (frozenset(game_state.x), frozenset(game_state.o))\n",
    "            hashable_next_state = (frozenset(next_state.x), frozenset(next_state.o))\n",
    "            existing_state = self.learning_dictionary[hashable_state]\n",
    "            if existing_state == 0:\n",
    "                self.learning_dictionary[hashable_state] = [[], []]\n",
    "\n",
    "            previous_moves: list = self.learning_dictionary[hashable_state][0]\n",
    "            if move in previous_moves:\n",
    "                i = previous_moves.index(move)\n",
    "                current_reward = self.learning_dictionary[hashable_state][1][i]\n",
    "                if self.learning_dictionary[hashable_next_state] != 0:\n",
    "                    optimal_next_reward = max(self.learning_dictionary[hashable_next_state][1])\n",
    "                else:\n",
    "                    optimal_next_reward = 0\n",
    "                # self.learning_dictionary[hashable_state][1][i] = previous_reward * self.learning_rate + (reward - previous_reward) * self.discount_factor\n",
    "                self.learning_dictionary[hashable_state][1][i] = current_reward * (1 - self.learning_rate) + self.learning_rate * (reward + self.discount_factor * optimal_next_reward)\n",
    "            else:\n",
    "                move_reward = reward * self.discount_factor\n",
    "                self.learning_dictionary[hashable_state][0].append(move)\n",
    "                self.learning_dictionary[hashable_state][1].append(move_reward)\n",
    "        \n",
    "    def name(self):\n",
    "        return 'Q_learning_player'\n",
    "    \n",
    "    def data(self):\n",
    "        return self.role, self.learning_rate, self.discount_factor, self.exploration_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we play the game and every time the learning system makes a move it saves the state and the move it took in the game_log for the learning process\n",
    "After every game, if the player won or lost it gains a positive or negative reward associated with the moves it took at a given state (in case of a draw it learns nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(players, epochs, learning_player):\n",
    "    if learning_player == 'x':\n",
    "        learning_player = 0\n",
    "    if learning_player == 'o':\n",
    "        learning_player = 1\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        game_over = False\n",
    "        game_log = []\n",
    "        game_state = State(set(), set())\n",
    "        player_turn = 0\n",
    "        while not game_over:\n",
    "\n",
    "            move = players[player_turn].make_move(game_state)\n",
    "            if learning_player == player_turn:\n",
    "                current_state = deepcopy(game_state)\n",
    "            if player_turn == 0:\n",
    "                game_state.x.add(move)\n",
    "            else:\n",
    "                game_state.o.add(move)\n",
    "            if learning_player == player_turn:\n",
    "                game_log.append((current_state, move, deepcopy(game_state)))\n",
    "\n",
    "            if winning_state(game_state) or len(available_moves(game_state)) == 0:\n",
    "                game_over = True\n",
    "            \n",
    "            player_turn = 1 - player_turn\n",
    "\n",
    "        if winning_state(game_state):\n",
    "            players[learning_player].update(game_log, winning_state(game_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function to run a normal (non training) game, used to test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game(players, show=False):\n",
    "    game_state = State(set(), set())\n",
    "\n",
    "    move_counter = 0\n",
    "    while True:\n",
    "        move = players[0].make_move(game_state)\n",
    "        game_state.x.add(move)\n",
    "        move_counter += 1\n",
    "        if show:\n",
    "            print(f'Player X, move {move_counter}')\n",
    "            print_board(game_state)\n",
    "\n",
    "        if winning_state(game_state):\n",
    "            if show:\n",
    "                print(f'player X won')\n",
    "            return winning_state(game_state)\n",
    "\n",
    "        if len(available_moves(game_state)) == 0:\n",
    "            if show:\n",
    "                print('Draw')\n",
    "            return winning_state(game_state)\n",
    "\n",
    "        move = players[0].make_move(game_state)\n",
    "        game_state.o.add(move)\n",
    "        move_counter += 1\n",
    "        if show:\n",
    "            print(f'Player O, move {move_counter}')\n",
    "            print_board(game_state)\n",
    "\n",
    "        if winning_state(game_state):\n",
    "            if show:\n",
    "                print(f'player O won')\n",
    "            return winning_state(game_state)\n",
    "    \n",
    "    \n",
    "def testing(players, n_tests, learning_player):\n",
    "    if learning_player == 'x':\n",
    "        learning_player = 0\n",
    "    if learning_player == 'o':\n",
    "        learning_player = 1\n",
    "\n",
    "    wins = [0, 0, 0]\n",
    "    for _ in range(n_tests):\n",
    "        winner = game(players)\n",
    "        if winner == 1:\n",
    "            wins[0] += 1\n",
    "        if winner == -1:\n",
    "            wins[1] += 1\n",
    "        if winner == 0:\n",
    "            wins[2] += 1\n",
    "    print(f'Final results out of {n_tests} games: \\n\\t{players[0].name()} win rate: {round(wins[0]/n_tests*100, 2)} %\\n\\t{players[1].name()} win rate {round(wins[1]/n_tests*100, 2)} %\\n\\tDraw rate {round(wins[2]/n_tests*100, 2)} %')\n",
    "    return round(wins[learning_player]/n_tests*100, 2), players[learning_player].data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I run some trials with different values for learning rate, discount rate and exploration rate to find the best combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate, discount_factor, exploration_rate = 0.1, 0.1, 0.1\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 89.2 %\n",
      "\tRandom_player win rate 3.6 %\n",
      "\tDraw rate 7.2 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.1, 0.1, 0.1\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 59.0 %\n",
      "\tQ_learning_player win rate 27.9 %\n",
      "\tDraw rate 13.1 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.1, 0.1, 0.5\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 80.4 %\n",
      "\tRandom_player win rate 10.8 %\n",
      "\tDraw rate 8.8 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.1, 0.1, 0.5\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 57.8 %\n",
      "\tQ_learning_player win rate 28.5 %\n",
      "\tDraw rate 13.7 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.1, 0.1, 0.9\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 62.8 %\n",
      "\tRandom_player win rate 25.8 %\n",
      "\tDraw rate 11.4 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.1, 0.1, 0.9\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 58.7 %\n",
      "\tQ_learning_player win rate 28.7 %\n",
      "\tDraw rate 12.6 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.1, 0.5, 0.1\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 88.7 %\n",
      "\tRandom_player win rate 5.7 %\n",
      "\tDraw rate 5.6 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.1, 0.5, 0.1\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 56.9 %\n",
      "\tQ_learning_player win rate 29.0 %\n",
      "\tDraw rate 14.1 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.1, 0.5, 0.5\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 83.1 %\n",
      "\tRandom_player win rate 7.9 %\n",
      "\tDraw rate 9.0 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.1, 0.5, 0.5\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 59.9 %\n",
      "\tQ_learning_player win rate 28.2 %\n",
      "\tDraw rate 11.9 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.1, 0.5, 0.9\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 61.1 %\n",
      "\tRandom_player win rate 27.2 %\n",
      "\tDraw rate 11.7 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.1, 0.5, 0.9\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 56.9 %\n",
      "\tQ_learning_player win rate 29.5 %\n",
      "\tDraw rate 13.6 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.1, 0.9, 0.1\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 89.2 %\n",
      "\tRandom_player win rate 3.1 %\n",
      "\tDraw rate 7.7 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.1, 0.9, 0.1\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 60.1 %\n",
      "\tQ_learning_player win rate 28.7 %\n",
      "\tDraw rate 11.2 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.1, 0.9, 0.5\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 78.7 %\n",
      "\tRandom_player win rate 10.6 %\n",
      "\tDraw rate 10.7 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.1, 0.9, 0.5\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 59.3 %\n",
      "\tQ_learning_player win rate 27.3 %\n",
      "\tDraw rate 13.4 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.1, 0.9, 0.9\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 60.7 %\n",
      "\tRandom_player win rate 26.5 %\n",
      "\tDraw rate 12.8 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.1, 0.9, 0.9\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 57.5 %\n",
      "\tQ_learning_player win rate 29.3 %\n",
      "\tDraw rate 13.2 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.5, 0.1, 0.1\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 80.0 %\n",
      "\tRandom_player win rate 10.6 %\n",
      "\tDraw rate 9.4 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.5, 0.1, 0.1\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 56.2 %\n",
      "\tQ_learning_player win rate 30.5 %\n",
      "\tDraw rate 13.3 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.5, 0.1, 0.5\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 77.4 %\n",
      "\tRandom_player win rate 13.7 %\n",
      "\tDraw rate 8.9 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.5, 0.1, 0.5\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 56.9 %\n",
      "\tQ_learning_player win rate 29.0 %\n",
      "\tDraw rate 14.1 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.5, 0.1, 0.9\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 63.4 %\n",
      "\tRandom_player win rate 24.6 %\n",
      "\tDraw rate 12.0 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.5, 0.1, 0.9\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 57.9 %\n",
      "\tQ_learning_player win rate 30.0 %\n",
      "\tDraw rate 12.1 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.5, 0.5, 0.1\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 85.6 %\n",
      "\tRandom_player win rate 4.7 %\n",
      "\tDraw rate 9.7 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.5, 0.5, 0.1\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 56.5 %\n",
      "\tQ_learning_player win rate 29.7 %\n",
      "\tDraw rate 13.8 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.5, 0.5, 0.5\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 81.6 %\n",
      "\tRandom_player win rate 9.8 %\n",
      "\tDraw rate 8.6 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.5, 0.5, 0.5\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 59.9 %\n",
      "\tQ_learning_player win rate 28.9 %\n",
      "\tDraw rate 11.2 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.5, 0.5, 0.9\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 64.8 %\n",
      "\tRandom_player win rate 24.4 %\n",
      "\tDraw rate 10.8 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.5, 0.5, 0.9\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 59.8 %\n",
      "\tQ_learning_player win rate 26.4 %\n",
      "\tDraw rate 13.8 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.5, 0.9, 0.1\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 89.8 %\n",
      "\tRandom_player win rate 3.4 %\n",
      "\tDraw rate 6.8 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.5, 0.9, 0.1\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 57.3 %\n",
      "\tQ_learning_player win rate 29.0 %\n",
      "\tDraw rate 13.7 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.5, 0.9, 0.5\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 75.2 %\n",
      "\tRandom_player win rate 14.8 %\n",
      "\tDraw rate 10.0 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.5, 0.9, 0.5\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 55.8 %\n",
      "\tQ_learning_player win rate 31.0 %\n",
      "\tDraw rate 13.2 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.5, 0.9, 0.9\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 65.4 %\n",
      "\tRandom_player win rate 24.8 %\n",
      "\tDraw rate 9.8 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.5, 0.9, 0.9\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 60.2 %\n",
      "\tQ_learning_player win rate 27.8 %\n",
      "\tDraw rate 12.0 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.9, 0.1, 0.1\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 86.5 %\n",
      "\tRandom_player win rate 9.5 %\n",
      "\tDraw rate 4.0 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.9, 0.1, 0.1\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 60.1 %\n",
      "\tQ_learning_player win rate 26.6 %\n",
      "\tDraw rate 13.3 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.9, 0.1, 0.5\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 73.5 %\n",
      "\tRandom_player win rate 13.2 %\n",
      "\tDraw rate 13.3 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.9, 0.1, 0.5\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 58.2 %\n",
      "\tQ_learning_player win rate 29.8 %\n",
      "\tDraw rate 12.0 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.9, 0.1, 0.9\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 62.3 %\n",
      "\tRandom_player win rate 26.0 %\n",
      "\tDraw rate 11.7 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.9, 0.1, 0.9\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 55.8 %\n",
      "\tQ_learning_player win rate 30.5 %\n",
      "\tDraw rate 13.7 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.9, 0.5, 0.1\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 87.9 %\n",
      "\tRandom_player win rate 3.3 %\n",
      "\tDraw rate 8.8 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.9, 0.5, 0.1\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 59.0 %\n",
      "\tQ_learning_player win rate 29.4 %\n",
      "\tDraw rate 11.6 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.9, 0.5, 0.5\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 78.7 %\n",
      "\tRandom_player win rate 13.3 %\n",
      "\tDraw rate 8.0 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.9, 0.5, 0.5\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 57.3 %\n",
      "\tQ_learning_player win rate 29.1 %\n",
      "\tDraw rate 13.6 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.9, 0.5, 0.9\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 62.3 %\n",
      "\tRandom_player win rate 26.2 %\n",
      "\tDraw rate 11.5 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.9, 0.5, 0.9\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 55.6 %\n",
      "\tQ_learning_player win rate 31.3 %\n",
      "\tDraw rate 13.1 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.9, 0.9, 0.1\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 92.3 %\n",
      "\tRandom_player win rate 2.5 %\n",
      "\tDraw rate 5.2 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.9, 0.9, 0.1\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 57.7 %\n",
      "\tQ_learning_player win rate 29.3 %\n",
      "\tDraw rate 13.0 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.9, 0.9, 0.5\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 77.9 %\n",
      "\tRandom_player win rate 10.6 %\n",
      "\tDraw rate 11.5 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.9, 0.9, 0.5\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 56.5 %\n",
      "\tQ_learning_player win rate 29.8 %\n",
      "\tDraw rate 13.7 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.9, 0.9, 0.9\n",
      "playing as = x\n",
      "Final results out of 1000 games: \n",
      "\tQ_learning_player win rate: 60.1 %\n",
      "\tRandom_player win rate 27.2 %\n",
      "\tDraw rate 12.7 %\n",
      "\n",
      "\n",
      "learning_rate, discount_factor, exploration_rate = 0.9, 0.9, 0.9\n",
      "playing as = o\n",
      "Final results out of 1000 games: \n",
      "\tRandom_player win rate: 59.7 %\n",
      "\tQ_learning_player win rate 28.1 %\n",
      "\tDraw rate 12.2 %\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "best results when playing as X (1st player): 92.3\n",
      "obtained with learning_rate, discount_factor, exploration_rate = 0.9, 0.9, 0.1\n",
      "best results when playing as O (2nd player): 31.3\n",
      "obtained with learning_rate, discount_factor, exploration_rate = 0.9, 0.5, 0.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results_x = []\n",
    "results_o = []\n",
    "#lr, df, er = 0.1, 0.9, 0.1\n",
    "for lr in [0.1, 0.5, 0.9]:\n",
    "    for df in [0.1, 0.5, 0.9]:\n",
    "        for er in [0.1, 0.5, 0.9]:\n",
    "            for role in ['x', 'o']:\n",
    "                print(f'learning_rate, discount_factor, exploration_rate = {lr}, {df}, {er}\\nplaying as = {role}')\n",
    "                player_1 = Q_learning_player(lr, df, er, role)\n",
    "                player_2 = random_player()\n",
    "                train_epochs = 100_000\n",
    "                test_epochs = 1_000\n",
    "                if role == 'x':\n",
    "                    players = [player_1, player_2]\n",
    "                elif role == 'o':\n",
    "                    players = [player_2, player_1]\n",
    "\n",
    "                training(players, train_epochs, role)\n",
    "\n",
    "                win_rate, data = testing(players, test_epochs, role)\n",
    "                if data[0] == 'x':\n",
    "                    results_x.append([win_rate, data])\n",
    "                if data[0] == 'o':\n",
    "                    results_o.append([win_rate, data])\n",
    "                print('\\n')\n",
    "\n",
    "print('----------------------------------------')\n",
    "best = max(sorted(results_x, key=lambda x: x[0]))\n",
    "print(f'best results when playing as X (1st player): {best[0]}')\n",
    "print(f'obtained with learning_rate, discount_factor, exploration_rate = {best[1][1]}, {best[1][2]}, {best[1][3]}')\n",
    "\n",
    "best = max(sorted(results_o, key=lambda x: x[0]))\n",
    "print(f'best results when playing as O (2nd player): {best[0]}')\n",
    "print(f'obtained with learning_rate, discount_factor, exploration_rate = {best[1][1]}, {best[1][2]}, {best[1][3]}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running some more in depth tests (more training epochs) for the best results we got above, just to make sure it wasn't a fluke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate, discount_factor, exploration_rate = 0.9, 0.9, 0.1\n",
      "playing as = x\n",
      "Final results out of 100000 games: \n",
      "\tQ_learning_player win rate: 87.6 %\n",
      "\tRandom_player win rate 2.56 %\n",
      "\tDraw rate 9.84 %\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr, df, er, role = 0.9, 0.9, 0.1, 'x'\n",
    "print(f'learning_rate, discount_factor, exploration_rate = {lr}, {df}, {er}\\nplaying as = {role}')\n",
    "player_1 = Q_learning_player(lr, df, er, role)\n",
    "player_2 = random_player()\n",
    "train_epochs = 1_000_000\n",
    "test_epochs = 100_000\n",
    "if role == 'x':\n",
    "    players = [player_1, player_2]\n",
    "elif role == 'o':\n",
    "    players = [player_2, player_1]\n",
    "\n",
    "training(players, train_epochs, role)\n",
    "testing(players, test_epochs, role)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "There clearly is some issue when going second (playing as o) but as of writing this I'm not sure what it is.\n",
    "Because of the deadline I'll leave these results as they are but I'll continue on my own time to work on them to try and fix this issue\n",
    "\n",
    "(my thoughts is there is some error in how I swap the objective of the learning player when I make it play as 'o' by having it minimize the reward instead of maximizing them)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
